{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a9d69-8b89-4123-9086-857c8d7e91f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import functools\n",
    "import time\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#%matplotlib inline\n",
    "torch.manual_seed(1)\n",
    "#%config InlineBackend.figure_format = 'svg' # 'svg', 'retina'\n",
    "\n",
    "# Params \n",
    "MIN_POINTS = 300\n",
    "IMG_H = 374\n",
    "IMG_W = 1238\n",
    "batch_size = 64\n",
    "\n",
    "# Data folders\n",
    "PC_DIR = './data/processed/{}/training/point_clouds/'.format(MIN_POINTS)\n",
    "IMG_DIR = './data/processed/{}/training/image_2/'.format(MIN_POINTS)\n",
    "CENTERS_DIR = './data/processed/{}/training/centers/'.format(MIN_POINTS)\n",
    "CALIB_DIR = './data/processed/{}/training/calib/'.format(MIN_POINTS)\n",
    "\n",
    "# Point cloud to image projection\n",
    "def pcd_to_img(pcd, index):\n",
    "    FRAME = f'{\"0\"*(6 - len(str(index)))}{index}'\n",
    "    # Load binary point cloud\n",
    "\n",
    "    with open(f'{CALIB_DIR}{FRAME}.txt', 'r') as f:\n",
    "        calib = f.readlines()\n",
    "\n",
    "    P2 = np.array([float(x) for x in calib[2].strip('\\n').split(' ')[1:]]).reshape(3, 4)\n",
    "    R0_rect = np.array([float(x) for x in calib[4].strip('\\n').split(' ')[1:]]).reshape(3,3)\n",
    "    # Add a 1 in bottom-right, reshape to 4 x 4\n",
    "    R0_rect = np.insert(R0_rect,3,values=[0,0,0],axis=0)\n",
    "    R0_rect = np.insert(R0_rect,3,values=[0,0,0,1],axis=1)\n",
    "    Tr_velo_to_cam = np.array([float(x) for x in calib[5].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    Tr_velo_to_cam = np.insert(Tr_velo_to_cam,3,values=[0,0,0,1],axis=0)\n",
    "    # Reshape and drop reflection values\n",
    "    pcd = pcd.reshape((-1, 4))\n",
    "    points, reflectances = pcd[:, 0:3], pcd[:, 3]\n",
    "    velo = np.insert(points,3,1,axis=1).T\n",
    "    #cam = P2 * R0_rect * Tr_velo_to_cam * velo\n",
    "    cam = P2.dot(R0_rect.dot(Tr_velo_to_cam.dot(velo)))\n",
    "    cam[:2] /= cam[2,:]\n",
    "    #velo = np.delete(velo,np.where(velo[0,:]<0),axis=1)\n",
    "    return cam, reflectances\n",
    "\n",
    "# Embed point cloud projection in camera image\n",
    "def make_hybrid_image(border_size, pcd, img):\n",
    "    \"\"\" Place projected point cloud image inside the RGB image \"\"\"\n",
    "    \n",
    "    imsize = img.shape[-1]\n",
    "    hybrid_img = img.detach().clone()\n",
    "    pcd_center = pcd.detach().clone()[:, :, \n",
    "                                      border_size:imsize-border_size,\n",
    "                                      border_size:imsize-border_size\n",
    "                                     ]\n",
    "    hybrid_img[:, :, border_size:imsize-border_size, \n",
    "               border_size:imsize-border_size] \\\n",
    "    = pcd_center.clone()\n",
    "    return hybrid_img\n",
    "\n",
    "# Visualization\n",
    "def show_results(data, clip=4, save=False):\n",
    "    \"\"\" Shows images\n",
    "    parameters:\n",
    "        data: list -- [input: torch.Tensor, output: \", truth: \"]\n",
    "        clip: int  -- The number of images in the batch to show\n",
    "    \"\"\"\n",
    "    titles = ['in', 'out', 'true']\n",
    "    \n",
    "    # Don't try to show more than exists\n",
    "    if clip > data[0].shape[0]:\n",
    "        clip = data[0].shape[0]\n",
    "    \n",
    "    \n",
    "    for j in range(clip):\n",
    "        fig, axes = plt.subplots(1, 3, sharey=True, figsize=(11,4))\n",
    "        fig.suptitle('Lidar point cloud -> RGB', color=\"white\")\n",
    "        fig.patch.set_facecolor('black')\n",
    "\n",
    "        for i in range(3):\n",
    "            ax = axes[i]\n",
    "            ax.imshow(((data[i][j].numpy().transpose(1, 2, 0) + 1)*127.5).astype('uint8'))\n",
    "            ax.set_title(titles[i], color=\"white\")\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.savefig(str(j)+'.png', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6685a3-08ea-443e-a700-8cc758a25a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset classes\n",
    "class ProjectedPointCloudDataset(Dataset):\n",
    "    def __init__(self, folder, trans):\n",
    "        self.folder = folder\n",
    "        self.files = glob.glob(os.path.join(folder, '*.bin'))\n",
    "        self.trans = trans\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # little-endian float32\n",
    "        pc = np.fromfile(os.path.join(self.folder, \n",
    "                              '{0:06d}.bin'.format(index)), '<f4')\n",
    "        cam, refl = pcd_to_img(pc, index)\n",
    "        [cH, cW] = np.load(os.path.join(CENTERS_DIR, \n",
    "                                        '{0:06d}.npy'.format(index)\n",
    "                                       )\n",
    "                          )\n",
    "        # project 3d point cloud onto 2d\n",
    "        u, v, z = cam\n",
    "        # translate 2d image\n",
    "        u -= cW - 128\n",
    "        v -= cH - 128\n",
    "        proj_mat = np.zeros((IMG_H, IMG_W))\n",
    "        # populate matrix\n",
    "        for idx in range(u.shape[0]):\n",
    "            row = int(v[idx])\n",
    "            col = int(u[idx])\n",
    "            proj_mat[row, col] = z[idx]\n",
    "            \n",
    "        mat256 = proj_mat[:256, :256]\n",
    "        \n",
    "        pm3c = np.stack((mat256, \n",
    "                         mat256, \n",
    "                         mat256))\n",
    "        # normalize to [-1, 1] \n",
    "        pm3c = torch.Tensor(pm3c)\n",
    "        if self.trans:\n",
    "            pm3c = self.trans(pm3c)\n",
    "        pm3c =  2*(pm3c - pm3c.min())/(pm3c.max()-pm3c.min()) \n",
    "        pm3c -= 1\n",
    "        return pm3c\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "class KittiLeftColorDataset(Dataset):\n",
    "    \"\"\" Kitti Left Color (image_2) dataset\"\"\"\n",
    "    def __init__(self, folder, trans=None):\n",
    "        self.folder = folder\n",
    "        self.files = glob.glob(os.path.join(folder, '*.png'))\n",
    "        self.trans = trans\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.folder, \n",
    "                                f'{\"0\"*(6 - len(str(index)))}{index}.png')\n",
    "        img = torch.Tensor(io.imread(img_path).transpose(2, 0, 1))\n",
    "        if self.trans:\n",
    "            img = self.trans(img)\n",
    "        img = (img/127.5)\n",
    "        img -= 1\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Concatenate several datasets \"\"\"\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c85614-7b20-44f2-a464-471dd3365f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/da39a525eb793614807db4330cfe9b2157bbe33a/models/networks.py#L539\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b357fd-dd94-40f7-ae93-7133163f8eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/truthisneverlinear/attention-u-net-pytorch#Attention-U-Net\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                                  nn.Conv2d(ch_in, ch_out,\n",
    "                                            kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True),\n",
    "                                  nn.BatchNorm2d(ch_out),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Conv2d(ch_out, ch_out,\n",
    "                                            kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True),\n",
    "                                  nn.BatchNorm2d(ch_out),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(ch_in, ch_out,\n",
    "                                         kernel_size=3,stride=1,\n",
    "                                         padding=1, bias=True),\n",
    "                                nn.BatchNorm2d(ch_out),\n",
    "                                nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, f_g, f_l, f_int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_g = nn.Sequential(\n",
    "                                nn.Conv2d(f_g, f_int,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0, bias=True),\n",
    "                                nn.BatchNorm2d(f_int)\n",
    "        )\n",
    "        \n",
    "        self.w_x = nn.Sequential(\n",
    "                                nn.Conv2d(f_l, f_int,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0, bias=True),\n",
    "                                nn.BatchNorm2d(f_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "                                nn.Conv2d(f_int, 1,\n",
    "                                         kernel_size=1, stride=1,\n",
    "                                         padding=0,  bias=True),\n",
    "                                nn.BatchNorm2d(1),\n",
    "                                nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.w_g(g)\n",
    "        x1 = self.w_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return psi*x\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, n_classes=1, in_channel=6, out_channel=3):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv1 = ConvBlock(ch_in=in_channel, ch_out=64)\n",
    "        self.conv2 = ConvBlock(ch_in=64, ch_out=128)\n",
    "        self.conv3 = ConvBlock(ch_in=128, ch_out=256)\n",
    "        self.conv4 = ConvBlock(ch_in=256, ch_out=512)\n",
    "        self.conv5 = ConvBlock(ch_in=512, ch_out=1024)\n",
    "        \n",
    "        self.up5 = UpConvBlock(ch_in=1024, ch_out=512)\n",
    "        self.att5 = AttentionBlock(f_g=512, f_l=512, f_int=256)\n",
    "        self.upconv5 = ConvBlock(ch_in=1024, ch_out=512)\n",
    "        \n",
    "        self.up4 = UpConvBlock(ch_in=512, ch_out=256)\n",
    "        self.att4 = AttentionBlock(f_g=256, f_l=256, f_int=128)\n",
    "        self.upconv4 = ConvBlock(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.up3 = UpConvBlock(ch_in=256, ch_out=128)\n",
    "        self.att3 = AttentionBlock(f_g=128, f_l=128, f_int=64)\n",
    "        self.upconv3 = ConvBlock(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.up2 = UpConvBlock(ch_in=128, ch_out=64)\n",
    "        self.att2 = AttentionBlock(f_g=64, f_l=64, f_int=32)\n",
    "        self.upconv2 = ConvBlock(ch_in=128, ch_out=64)\n",
    "        \n",
    "        self.conv_1x1 = nn.Conv2d(64, out_channel,\n",
    "                                  kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x1 = self.conv1(x)\n",
    "        \n",
    "        x2 = self.maxpool(x1)\n",
    "        x2 = self.conv2(x2)\n",
    "        \n",
    "        x3 = self.maxpool(x2)\n",
    "        x3 = self.conv3(x3)\n",
    "        \n",
    "        x4 = self.maxpool(x3)\n",
    "        x4 = self.conv4(x4)\n",
    "        \n",
    "        x5 = self.maxpool(x4)\n",
    "        x5 = self.conv5(x5)\n",
    "        \n",
    "        # decoder + concat\n",
    "        d5 = self.up5(x5)\n",
    "        x4 = self.att5(g=d5, x=x4)\n",
    "        d5 = torch.concat((x4, d5), dim=1)\n",
    "        d5 = self.upconv5(d5)\n",
    "        \n",
    "        d4 = self.up4(d5)\n",
    "        x3 = self.att4(g=d4, x=x3)\n",
    "        d4 = torch.concat((x3, d4), dim=1)\n",
    "        d4 = self.upconv4(d4)\n",
    "        \n",
    "        d3 = self.up3(d4)\n",
    "        x2 = self.att3(g=d3, x=x2)\n",
    "        d3 = torch.concat((x2, d3), dim=1)\n",
    "        d3 = self.upconv3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        x1 = self.att2(g=d2, x=x1)\n",
    "        d2 = torch.concat((x1, d2), dim=1)\n",
    "        d2 = self.upconv2(d2)\n",
    "        \n",
    "        d1 = self.conv_1x1(d2)\n",
    "        \n",
    "        return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284dc6e0-c77c-4958-8297-82c0c23da3b4",
   "metadata": {},
   "source": [
    "# Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75542788-f4f3-4610-a41a-39b32abcbd41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_point_clouds = ProjectedPointCloudDataset(PC_DIR, trans = transforms.Resize(64, antialias=True))\n",
    "images = KittiLeftColorDataset(IMG_DIR, trans = transforms.Resize(64, antialias=True))\n",
    "dataset = ConcatDataset(images, proj_point_clouds)\n",
    "\n",
    "split_fracs = [0.8, 0.2]\n",
    "splits = [int(split_fracs[0]*len(dataset)), len(dataset) - int(split_fracs[0]*len(dataset))]\n",
    "train_data, test_data = random_split(dataset, splits)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=3,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=True\n",
    "                                          )\n",
    "n_batches = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50ec2b-3332-4c44-8742-7e68620928b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load saved generator\n",
    "G_Net = AttentionUNet()\n",
    "G_Net.load_state_dict(torch.load(\"G_30\"))\n",
    "G_Net.to(device)\n",
    "D_Net = NLayerDiscriminator(3)\n",
    "D_Net.load_state_dict(torch.load(\"D_30\"))\n",
    "D_Net.to(device)\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "L1 = L1Loss()\n",
    "\n",
    "# Convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "lr = 0.00002\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(D_Net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(G_Net.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e79ca-b817-4977-9fd5-f267679247e2",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346ecda-1836-47d0-a9dc-b5aecf703076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(100):\n",
    "    G_err_avg = 0.\n",
    "    D_err_avg = 0.\n",
    "    t_start = time.time()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        img_batch = batch[0].to(device)\n",
    "        pc_batch = batch[1].to(device)\n",
    "        hyb_batch = make_hybrid_image(5, pc_batch, img_batch)\n",
    "        \n",
    "        G_Net.zero_grad()\n",
    "        D_Net.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            t = torch.full((hyb_batch.shape[0],), 0, device=device)\n",
    "            noise = torch.randn_like(hyb_batch).to(device)\n",
    "            hyb_batch = torch.cat([noise, hyb_batch], 1)\n",
    "            # Discriminator optimization\n",
    "            # Generate fake\n",
    "            fake_batch = G_Net(hyb_batch)\n",
    "            \n",
    "            # Classify real\n",
    "            D_out_real = D_Net(img_batch).view(-1)\n",
    "            # Classify fake\n",
    "            D_out_fake = D_Net(fake_batch).view(-1)\n",
    "        \n",
    "            D_target_real = torch.full(D_out_real.shape, real_label, device=device)\n",
    "            D_target_fake = torch.full(D_out_fake.shape, fake_label, device=device)\n",
    "    \n",
    "            D_err_real = criterion(D_out_real, D_target_real)\n",
    "            D_err_fake = criterion(D_out_fake, D_target_fake)\n",
    "            D_err = D_err_real + D_err_fake\n",
    "        \n",
    "        # Scaler\n",
    "        scaler.scale(D_err).backward(retain_graph=True)\n",
    "        scaler.step(optimizerD)\n",
    "        \n",
    "        # No scaler\n",
    "        #D_err.backward()\n",
    "        #optimizerD.step()\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            # Generator optimization\n",
    "            fake_batch = G_Net(hyb_batch)\n",
    "            D_out_fake = D_Net(fake_batch).view(-1)\n",
    "            \n",
    "            G_target = torch.full(D_out_fake.shape, real_label, device=device)\n",
    "            \n",
    "            G_err = criterion(D_out_fake, G_target) + 100*L1(fake_batch, img_batch)\n",
    "        # Scaler\n",
    "        scaler.scale(G_err).backward()\n",
    "        scaler.step(optimizerG)\n",
    "        scaler.update()\n",
    "        \n",
    "        # No scaler\n",
    "        #G_err.backward()\n",
    "        #optimizerG.step()\n",
    "        \n",
    "        with open('p2p_losses_.txt', 'a') as f:\n",
    "            f.write(f'{epoch}, {i}, {D_err}, {G_err}\\n')\n",
    "        t_end = time.time()\n",
    "        print(D_err.item(), G_err.item())\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(G_Net.state_dict(), \"G\")\n",
    "        torch.save(D_Net.state_dict(), \"D\")\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        clear_output(wait=True)\n",
    "        display_data = [hyb_batch[:, 3:, :, :].detach().cpu(), \n",
    "                        fake_batch.detach().cpu(), \n",
    "                        img_batch.detach().cpu()]\n",
    "        show_results(display_data, clip=4, save=False)\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11630ad-e410-4126-af11-78f656ed41c3",
   "metadata": {},
   "source": [
    "# Generate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394bde5-c368-4e5a-822b-de939449e5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=3,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=True\n",
    "                                        )\n",
    "\n",
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4fd8c-6f7d-45b1-ae12-2e4e055a4241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_batch, proj_batch = next(test_iter)\n",
    "with torch.no_grad():\n",
    "    hyb_batch = make_hybrid_image(5, proj_batch, img_batch).to(device)\n",
    "    noise = torch.randn_like(hyb_batch).to(device)\n",
    "    hyb_batch = torch.cat([noise, hyb_batch], 1)\n",
    "    generated = G_Net(hyb_batch)\n",
    "\n",
    "display_data = [hyb_batch[:,3:,:,:].detach().cpu(), \n",
    "                generated.detach().cpu(), \n",
    "                img_batch.detach().cpu()]\n",
    "show_results(display_data, clip=100, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38cfb3-e036-4796-a187-4706519e0954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "with open('p2p_losses_.txt') as f:\n",
    "    lines = f.readlines()\n",
    "p2p_losses = []\n",
    "for line in lines:\n",
    "    line = line.strip('\\n').split(',')\n",
    "    D_loss = float(line[2]) \n",
    "    G_loss = float(line[3])\n",
    "    p2p_losses.append([D_loss, G_loss])\n",
    "    \n",
    "p2p_losses = np.array(p2p_losses)\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('iteration')\n",
    "ax1.set_ylabel('Discriminator', color=color)\n",
    "ax1.plot(p2p_losses[:, 0], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Generator', color=color)  \n",
    "ax2.plot(p2p_losses[:,1], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_title('Learning curves')\n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
